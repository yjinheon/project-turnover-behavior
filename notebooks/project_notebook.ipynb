{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019 GOMS 설문조사를 바탕사회 초년생의 이직의도에 영향을 미치는 요인 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install brewer2mpl\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# warning 삭제용\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "large = 22; med = 16; small = 12\n",
    "params = {'axes.titlesize': large,\n",
    "          'legend.fontsize': med,\n",
    "          'figure.figsize': (16, 10),\n",
    "          'axes.labelsize': med,\n",
    "          'axes.titlesize': med,\n",
    "          'xtick.labelsize': med,\n",
    "          'ytick.labelsize': med,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../src')\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g181d001</th>\n",
       "      <th>g181d006</th>\n",
       "      <th>g181e001</th>\n",
       "      <th>g181pid</th>\n",
       "      <th>g181majorcat</th>\n",
       "      <th>g181sex</th>\n",
       "      <th>g181birthy</th>\n",
       "      <th>g181age</th>\n",
       "      <th>g181graduy</th>\n",
       "      <th>g181a001</th>\n",
       "      <th>...</th>\n",
       "      <th>g181q021</th>\n",
       "      <th>g181q022</th>\n",
       "      <th>g181q023</th>\n",
       "      <th>g181p001</th>\n",
       "      <th>g181p008</th>\n",
       "      <th>g181p036</th>\n",
       "      <th>g181p046</th>\n",
       "      <th>g181p041</th>\n",
       "      <th>worker_type</th>\n",
       "      <th>regular_worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100036</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997</td>\n",
       "      <td>21.750000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100047</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1994</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100049</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1995</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100061</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18152</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>913797</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1993</td>\n",
       "      <td>25.833333</td>\n",
       "      <td>2017</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18156</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>913858</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>913874</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1993</td>\n",
       "      <td>25.833333</td>\n",
       "      <td>2017</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>913907</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>25.083333</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18162</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>990019</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>27.166667</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>employed</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10525 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       g181d001  g181d006  g181e001  g181pid  g181majorcat  g181sex  \\\n",
       "3           NaN       NaN       2.0   100034             1        2   \n",
       "4           NaN       NaN       2.0   100036             1        1   \n",
       "5           NaN       NaN       2.0   100047             1        1   \n",
       "6           1.0       2.0       2.0   100049             1        1   \n",
       "7           NaN       NaN       2.0   100061             1        1   \n",
       "...         ...       ...       ...      ...           ...      ...   \n",
       "18152       NaN       NaN       2.0   913797             7        2   \n",
       "18156       NaN       NaN       2.0   913858             7        2   \n",
       "18157       NaN       NaN       2.0   913874             7        2   \n",
       "18159       NaN       NaN       2.0   913907             7        2   \n",
       "18162       NaN       NaN       2.0   990019             3        1   \n",
       "\n",
       "       g181birthy    g181age  g181graduy  g181a001  ...  g181q021  g181q022  \\\n",
       "3            1994  25.583333        2018    2018.0  ...         2         5   \n",
       "4            1997  21.750000        2018    2018.0  ...         4         3   \n",
       "5            1994  25.250000        2018    2018.0  ...         4         4   \n",
       "6            1995  24.500000        2018    2018.0  ...         6         4   \n",
       "7            1993  26.000000        2018    2018.0  ...         4         1   \n",
       "...           ...        ...         ...       ...  ...       ...       ...   \n",
       "18152        1993  25.833333        2017    2016.0  ...         4         4   \n",
       "18156        1995  24.000000        2018    2018.0  ...         3         2   \n",
       "18157        1993  25.833333        2017    2018.0  ...         2         2   \n",
       "18159        1994  25.083333        2018    2018.0  ...         4         4   \n",
       "18162        1992  27.166667        2018    2018.0  ...         2         1   \n",
       "\n",
       "       g181q023  g181p001  g181p008  g181p036  g181p046  g181p041  \\\n",
       "3             1         1         2         5         1         2   \n",
       "4             4         1         2         4         1         2   \n",
       "5             4         1         2         3         1         2   \n",
       "6             2         1         2         4         1         2   \n",
       "7             4         1         2         4         3         2   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "18152         4         1         2         5         1         2   \n",
       "18156         3         1         2         2         1         2   \n",
       "18157         1         1         2         4         1         2   \n",
       "18159         5         1         2         3         1         2   \n",
       "18162         1         1         2         2         1         2   \n",
       "\n",
       "       worker_type  regular_worker  \n",
       "3         employed             yes  \n",
       "4         employed             yes  \n",
       "5         employed             yes  \n",
       "6         employed             yes  \n",
       "7         employed             yes  \n",
       "...            ...             ...  \n",
       "18152     employed             yes  \n",
       "18156     employed             yes  \n",
       "18157     employed             yes  \n",
       "18159     employed             yes  \n",
       "18162     employed             yes  \n",
       "\n",
       "[10525 rows x 83 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.py\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8262\n",
       "1    2263\n",
       "Name: g181a297, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['g181a297'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.get_dataset_names()\n",
    "#df = sns.load_dataset(\"titanic\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"g181a297\"\n",
    "#target = \"survived\"\n",
    "\n",
    "def split(df, target, test_size=0.3):\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(df, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import OrdinalEncoder\n",
    "# column tranformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict =  {\n",
    "      'g181pid':'id', # id \n",
    "       'g181majorcat':'majorcat', # 전공계열\n",
    "       'g181sex':'sex_cat', # 성별   \n",
    "       'g181birthy':'birth_date', # 출생년\n",
    "       'g181age':'age_num', # 연령\n",
    "       'g181graduy':'graduy_date', #졸업년\n",
    "### 사업체 관련       \n",
    "        'g181a001':'year_start_date', # 현일자리 시작년\n",
    "        'g181a002':'month_start_date', # 현일자리 시작월\n",
    "        'g181a004_10':'ind_cat', # 일자리 산업 대분류\n",
    "        'g181a010':'corp_worker_cat', # 기업체 종사자 수 # 결측값문제로 categorical로 변경\n",
    "        'g181a011':'biz_worker_cat', # 사업체 종사자 수\n",
    "        'g181a018':'tw_hour', # 출근시간_시간\n",
    "        'g181a019':'tw_min', # 출근시간_분\n",
    "        'g181a116':'workday_num', # 주당 정규 근로일\n",
    "        'g181a117':'worktime_num', # 주당정규 근로시간\n",
    "        'g181a118':'worktime_ex_num', # 주당초과 근로시간\n",
    "        'g181a119': 'holy_work_num', # 월평균 휴일근로\n",
    "        'g181a020': 'biztype_cat', # 사업체형태\n",
    "       'g181a022':'regular_cat',     # 정규직 비정규직 여부\n",
    "### 혜택관련\n",
    "        'g181a390':'voluntary_cat',     # 일자리 형태 자발, 비자발 여부\n",
    "        'g181a035':'shift_cat' ,    # 교대제 여부\n",
    "        'g181a038':'pension_cat',     # 퇴직금 제공 여부\n",
    "        'g181a039':'payed_vc_cat',     # 제공여부 2- 유급휴가\n",
    "        'g181a043':'maternity_cat',     #  6- 육아휴직\n",
    "        'g181a045':'overtime_pay_cat',     # 8- 시간 외 수당\n",
    "        'g181a046':'bonus_cat',     # 9- 상여금\n",
    "        'g181a048':'weekly_hl_cat',     # 11- 유급주휴\n",
    "        'g181a392':'baby_vc_cat',     # 12- 산전후휴가\n",
    "        'g181a120':'wage_type_cat',     # 급여 형태 구분\n",
    "        'g181a122':'month_wage_num',     # 월 평균 근로소득\n",
    "        'g181a126':'sat_wage_num',     # 만족도-임금\n",
    "        'g181a127':'sat_stable_num',     # 만족도-고용안정성\n",
    "        'g181a128':'sat_work_num',     # 만족도-직무내용\n",
    "        'g181a129':'sat_env_num',     # 만족도-근무환경\n",
    "        'g181a130':'sat_wt_num',     # 만족도-노동시간\n",
    "        'g181a131':'sat_potential_num',     # 만족도-발전가능성\n",
    "        'g181a132':'sat_relation_num',     # 만족도-인간관계\n",
    "        'g181a133':'sat_welfare_num',     # 만족도-복리후생\n",
    "        'g181a134':'sat_hr_num',     # 만족도-인사체계\n",
    "        'g181a135':'sat_rep1_num',     # 만족도-사회적평판-일\n",
    "        'g181a136':'sat_auto_num',     # 만족도-자율성 및 권한\n",
    "        'g181a137':'sat_rep2_num',     # 만족도-일자리-사회적 평판\n",
    "        'g181a138':'sat_fit_num',     # 만족도-적성흥미일치도\n",
    "        'g181a139':'sat_edu_num',     # 만족도-직무관련 교육\n",
    "        'g181a140':'sat_general_num',     # 만족도-일자리_전반적만족도\n",
    "        'g181a141':'sat_work-general_num',     # 만족도-업무_전반적만족도\n",
    "        'g181a142':'edu-fit_num',     # 교육수준-일수준일치정도\n",
    "        'g181a143':'skill-fit_num',     # 일기술수준-본인기술수준일치정도\n",
    "        'g181a144':'major-fit_num',     # 주전공일치정도\n",
    "        'g181a146':'major_help_num',     # 전공지식업무도움정도\n",
    "        'g181a158':'ins_1_num',     # 보험-국민연금\n",
    "        'g181a159':'ins_2_num',     # 보험-특수직역연금\n",
    "        'g181a160':'ins_3_num',     # 보험-건강보험\n",
    "        'g181a161':'ins_4_num',     # 보험-고용보험\n",
    "        'g181a162':'ins_5_num',     # 보험-산재보험\n",
    "        'g181a189':'seeking_time_num',     # 구직활동경험기간-개월\n",
    "        'g181a283':'adjust_difficulty_cat',     # 다른일자리제의여부\n",
    "        'g181a285':'job_offer_cat',     # 적응시어려움여부\n",
    "        'g181a297':'turnover_intention',     # 이직준비 여부: target\n",
    "        'g181g001':'graduate_cat',     # 대학원 경험유무\n",
    "        'g181l001':'train_cat',     # 취업훈련경험유무\n",
    "        'g181q001':'health_num',     # 현재 견강상태\n",
    "        'g181q004':'smoke_cat',     # 흡연여부\n",
    "        'g181q006':'drink_num,',     # 음주빈도\n",
    "        'g181q015':'lifesat_personal',     # 삶의만족도-개인적 측면\n",
    "        'g181q016':'lifesat_relational',     # 삶의만족도-관계적 측면\n",
    "        'g181q017':'lifesat_group',     # 삶의만족도-소속집단\n",
    "        'g181q018':'emg_joy_num',     # 감정빈도-즐거운\n",
    "        'g181q019':'emg_happy_num',     # 감정빈도-행복한\n",
    "        'g181q020':'emg_comfort_num',     # 감정빈도-편안한\n",
    "        'g181q021':'emb_irr_num',     # 감정빈도-짜증나는\n",
    "        'g181q022':'emb_negative_num',     # 감정빈도-부정적인\n",
    "        'g181q023':'emb_spiritless',     # 감정빈도-무기력한\n",
    "        'g181p001':'marriage_cat',     # 혼인여부\n",
    "        'g181p008':'child_cat',     # 부양자녀 유무\n",
    "        'g181p036':'parent_asset_cat',     # 부모님 자산규모\n",
    "        'g181p046':'livetype_cat',     # 거주형태\n",
    "        'g181p041':'support_cat',      # 경제적 지원여부\n",
    "        # subset에서 추가한 변수들\n",
    "        'worker_type':'worker_type_cat',\n",
    "        'regular_worker':'regular_worker_cat',\n",
    "        'turnover_exp':'turnover_exp_cat',\n",
    "        'work_exp':'work_exp_cat'        \n",
    "}\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Preprocess():\n",
    "    \"\"\"\n",
    "    X_train, X_test\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Preprocessing Class\")\n",
    "        self.numeric_df = None\n",
    "        self.cat_df = None\n",
    "        self.ordinal_col = ['biz_worker_cat','parent_asset_cat','turnover_exp_cat']\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_dtypes(self,data):\n",
    "        # onehot 할 피처\n",
    "        #self.numeric_df = data.select_dtypes(include=['number'])\n",
    "        #self.cat_df = data.select_dtypes(exclude=['number'])\n",
    "        self.cat_col = [col for col in data.columns.tolist() if col.endswith('cat') ] \n",
    "        self.onehot_col = [col for col in self.cat_col if col not in self.ordinal_col]\n",
    "        self.numeric_col = [col for col in data if col not in self.onehot_col and col not in self.ordinal_col]\n",
    "        \n",
    "                 \n",
    "    def engineer(self,data):\n",
    "        \n",
    "        # id 칼럼 제거\n",
    "        data = data.drop(['g181pid'], axis=1)\n",
    "    \n",
    "        # 이직경험 빈도 추가\n",
    "        turnover_cond = [\n",
    "        (data['g181d001'].isnull()==True ), # 첫 직장 전 취업한적 없음(알바포함)\n",
    "        (data['g181d001']==1) & (data['g181d006']==1), # 알바 경험 있음\n",
    "        (data['g181d001']==1) & (data['g181d006']==2) & (data['g181e001']==2), # 전직장 있음, 1번이직\n",
    "        (data['g181e001']== 1)] # 전직장 2개 이상 있음\n",
    "        choices = [0, 1, 2, 3]\n",
    "\n",
    "        data['turnover_exp'] = np.select(turnover_cond, choices,default=3)\n",
    "    \n",
    "        data.drop(['g181d001','g181d006','g181e001'], axis=1, inplace=True)\n",
    "\n",
    "        # 알바이외 근로경험 여부\n",
    "        data['work_exp'] = np.where(data['turnover_exp'].isin([0,1]),0,1)\n",
    "    \n",
    "        # 구직활동기간 결측값 0으로 처리\n",
    "    \n",
    "        data['g181a189'] = data['g181a189'].fillna(0)\n",
    "    \n",
    "        # feature name 변경 (map)\n",
    "    \n",
    "        data.columns = data.columns.map(feature_dict)\n",
    "    \n",
    "    \n",
    "        data = data.drop(data[data['month_wage_num'] == -1].index) # 급여 모르는 경우 제거\n",
    "    \n",
    "        data['work_year'] = 2019 - data['year_start_date']  # 근무기간\n",
    "    \n",
    "        data['work_time_num'] = data['tw_min'] + data['tw_hour'] # 출근소요시간\n",
    "    \n",
    "        # 보험 수\n",
    "        insurances_col = [col for col in data if col.startswith('ins')]\n",
    "        data['insurances_num'] = 0\n",
    "        for col in insurances_col:\n",
    "            data['temp'] = np.where(data[col] == 1, 1, 0)\n",
    "            data['insurances_num'] += data['temp']\n",
    "    \n",
    "        # 회사 전반적 만족도\n",
    "        biz_sat_col = [col for col in data if col.startswith('sat')]\n",
    "        data['biz_sat'] = data[biz_sat_col].sum(axis=1)\n",
    "    \n",
    "        # 긍정적 감정\n",
    "        pos_col = [col for col in data if col.startswith('emg') ] \n",
    "        data['pos'] = data[pos_col].sum(axis=1)\n",
    "    \n",
    "        # 부정적 감정\n",
    "        neg_col = [col for col in data if col.startswith('neg') ]\n",
    "        data['neg'] = data[neg_col].sum(axis=1)\n",
    "    \n",
    "        # 삶의 만족도\n",
    "        lifesat_col = [col for col in data if col.startswith('lifesat')]\n",
    "        data['lifesat'] = data[lifesat_col].sum(axis=1)\n",
    "        \n",
    "        # 혜택 수\n",
    "        benefit_col  =['pension_cat',     # 퇴직금 제공 여부\n",
    "            'payed_vc_cat',     # 제공여부 2- 유급휴가\n",
    "            'maternity_cat',     #  6- 육아휴직\n",
    "            'overtime_pay_cat',     # 8- 시간 외 수당\n",
    "            'bonus_cat',     # 9- 상여금\n",
    "            'weekly_hl_cat',     # 11- 유급주휴\n",
    "            'baby_vc_cat'] # 출산휴가\n",
    "\n",
    "        data['benefit_num'] = 0\n",
    "\n",
    "        for cols in benefit_col:\n",
    "            data['temp'] = np.where(data[cols]==1, 1, 0)\n",
    "            data['benefit_num'] += data['temp']\n",
    "        \n",
    "        # 결측값 0으로 처리(구직활동기간이 0이기에)    \n",
    "        data['seeking_time_num'] = data['seeking_time_num'].fillna(0)\n",
    "    \n",
    "        data.drop(columns='temp',axis=1,inplace=True) # temp 삭제\n",
    "        data.drop(data[data.month_wage_num==-1].index,inplace=True) # 급여 모르는 경우 제거\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def drop_col(self,data,remove_col):\n",
    "        \n",
    "        all_cols = [self.numeric_col,self.onehot_col,self.ordinal_col]\n",
    "        \n",
    "        for col in all_cols:\n",
    "            if remove_col in col:\n",
    "                col.remove(remove_col)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        data.drop(columns = remove_col,axis=1,inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "    \n",
    "    def base_pipline(self,data):\n",
    "        \n",
    "      \n",
    "        \"\"\"\n",
    "        - categorical_feature imputation 고려\n",
    "        - OneHotEncoder \n",
    "            - handle_unknown = 'ignore' 옵션 -> specifically useful if you don't know all possible categories\n",
    "            - sparse = False 옵션 -> 기본값은 True. 리턴값을 sparse matrix에서 array로 변환\n",
    "            \n",
    "                \n",
    "        ohe_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-2)),\n",
    "            ('one-hot', OneHotEncoder(use_cat_names=True, handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        ord_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-2)),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        num_pipeline = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='median')),\n",
    "            ('scale', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        \"\"\"\n",
    "        #make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "        ohe_pipeline = make_pipeline(SimpleImputer(strategy='constant', fill_value=-2),OneHotEncoder(use_cat_names=True, handle_unknown='ignore'))\n",
    "        ord_pipeline = make_pipeline(SimpleImputer(strategy='constant', fill_value=-2),OrdinalEncoder(handle_unknown='ignore'))\n",
    "        num_pipeline = make_pipeline(SimpleImputer(strategy='median'),StandardScaler())\n",
    "        preprocess_pipeline = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', num_pipeline, self.numeric_col),\n",
    "                ('ohe', ohe_pipeline, self.onehot_col),\n",
    "                ('ord', ord_pipeline,self.ordinal_col)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preprocessor=make_pipeline(preprocess_pipeline)\n",
    "        \n",
    "        preprocessor.fit(data)\n",
    "    \n",
    "        return preprocessor\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.DataFrame({'brand': ['aaaa', 'asdfasdf', 'sadfds', 'NaN'],\n",
    "                   'category': ['asdf', 'asfa', 'asdfas', 'as'],\n",
    "                   'num1': [1, 1, 0, 0],\n",
    "                   'target': [0.2, 0.11, 1.34, 1.123]})\n",
    "\n",
    "numeric_features = ['num1']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['brand', 'category']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor',  LinearRegression())])\n",
    "clf.fit(df.drop('target', 1), df['target'])\n",
    "\n",
    "clf.named_steps['preprocessor'].transformers_[1][1]\\\n",
    "   .named_steps['onehot'].get_feature_names(categorical_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def ce_pipeline(self,data):\n",
    "        \"\"\"\n",
    "        category encode를 활용한 전처리 파이프라인\n",
    "        \n",
    "        encoder_list = [ce.backward_difference.BackwardDifferenceEncoder, \n",
    "               ce.basen.BaseNEncoder,\n",
    "               ce.binary.BinaryEncoder,\n",
    "                ce.cat_boost.CatBoostEncoder,\n",
    "                ce.hashing.HashingEncoder,\n",
    "                ce.helmert.HelmertEncoder,\n",
    "                ce.james_stein.JamesSteinEncoder,\n",
    "                ce.one_hot.OneHotEncoder,\n",
    "                ce.leave_one_out.LeaveOneOutEncoder,\n",
    "                ce.m_estimate.MEstimateEncoder,\n",
    "                ce.ordinal.OrdinalEncoder,\n",
    "                ce.polynomial.PolynomialEncoder,\n",
    "                ce.sum_coding.SumEncoder,\n",
    "                ce.target_encoder.TargetEncoder,\n",
    "                ce.woe.WOEEncoder\n",
    "                ]\n",
    "for encoder in encoder_list:\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('woe', encoder())])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(n_estimators=500))])\n",
    "    \n",
    "    model = pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(encoder)\n",
    "    print(f1_score(y_test, y_pred, average='macro'))\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class PreprocessSelector():\n",
    "    \"\"\"\n",
    "    전처리 방식 선택\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data=None\n",
    "        self._preprocessor=Preprocess()\n",
    "\n",
    "    def strategy(self, data, strategy_type=\"strategy1\"):\n",
    "        self.data=data\n",
    "        if strategy_type=='strategy1':\n",
    "            self._strategy1()\n",
    "        elif strategy_type=='strategy2':\n",
    "            self._strategy2()\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def _base_strategy(self):\n",
    "        self.data=self._preprocessor.engineer(self.data)\n",
    "        self._preprocessor.get_dtypes(self.data)\n",
    "\n",
    "    def _strategy1(self):\n",
    "\n",
    "        self._base_strategy()\n",
    "        self.data=self._preprocessor.drop_col(self.data,\"corp_worker_cat\")\n",
    "        self.data=self._preprocessor.base_pipline(self.data)\n",
    "\n",
    "    \n",
    "\n",
    "    def _strategy2(self):\n",
    "        \"\"\"\n",
    "        유사한 feature 제외\n",
    "        \"\"\"\n",
    "        self._base_strategy()\n",
    "        #self.data = self._preprocessor.drop_col(self.data,\"corp_worker_cat\")\n",
    "        self.daa = self._preprocessor.drop_col(self.data,\"\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Class\n"
     ]
    }
   ],
   "source": [
    "temp_selector=PreprocessSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearchHelper():\n",
    "    def __init__(self):\n",
    "        print(\"GridSearchHelper Created\")\n",
    "\n",
    "        self.gridSearchCV=None\n",
    "        self.clf_and_params=[]\n",
    "        self._initialize_clf_and_params()\n",
    "\n",
    "    def _initialize_clf_and_params(self):\n",
    "        \n",
    "        clf=LogisticRegression()\n",
    "        params={'penalty':['l1', 'l2'],\n",
    "                'C':np.logspace(0, 4, 10)\n",
    "                }\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "        clf = SVC()\n",
    "        params = [ {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "        clf=DecisionTreeClassifier()\n",
    "        params={'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "          'min_samples_leaf':[1],\n",
    "          'random_state':[123]}\n",
    "        #Because of depricating warning for Decision Tree which is not appended.\n",
    "        #But it give high competion accuracy score. You can append when you run the kernel\n",
    "        self.clf_and_params.append((clf,params))\n",
    "\n",
    "        clf = RandomForestClassifier()\n",
    "        params = {'n_estimators': [4, 6, 9],\n",
    "              'max_features': ['log2', 'sqrt','auto'],\n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10],\n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "        #Because of depricating warning for RandomForestClassifier which is not appended.\n",
    "        #But it give high competion accuracy score. You can append when you run the kernel\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "    def fit_predict_save(self, X_train, X_test, y_train,strategy_type):\n",
    "        self.X_train=X_train\n",
    "        self.X_test=X_test\n",
    "        self.y_train=y_train\n",
    "        self.strategy_type=strategy_type\n",
    "\n",
    "        clf_and_params = self.get_clf_and_params()\n",
    "        models=[]\n",
    "        self.results={}\n",
    "        for clf, params in clf_and_params:\n",
    "            self.current_clf_name = clf.__class__.__name__\n",
    "            grid_search_clf = GridSearchCV(clf, params, cv=5)\n",
    "            grid_search_clf.fit(self.X_train, self.y_train)\n",
    "            self.Y_pred = grid_search_clf.predict(self.X_test)\n",
    "            clf_train_acc = round(grid_search_clf.score(self.X_train, self.y_train) * 100, 2)\n",
    "            print(self.current_clf_name, \" trained and used for prediction on test data...\")\n",
    "            self.results[self.current_clf_name]=clf_train_acc\n",
    "            # for ensemble\n",
    "            models.append(clf)\n",
    "\n",
    "            self.save_result()\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    def show_result(self):\n",
    "        for clf_name, train_acc in self.results.items():\n",
    "                  print(\"{} train accuracy is {:.3f}\".format(clf_name, train_acc))\n",
    "        \n",
    "    def save_result(self):\n",
    "        id_idx = list(range(1, len(self.Y_pred) + 1))\n",
    "        Submission = pd.DataFrame({'Id': id_idx,\n",
    "                                    'Survived': self.Y_pred})\n",
    "        file_name=\"{}_{}.csv\".format(self.strategy_type,self.current_clf_name.lower())\n",
    "        Submission.to_csv(file_name, index=False)\n",
    "\n",
    "        print(\"Submission saved file name: \",file_name)\n",
    "\n",
    "    def get_clf_and_params(self):\n",
    "\n",
    "        return self.clf_and_params\n",
    "\n",
    "    def add(self,clf, params):\n",
    "        self.clf_and_params.append((clf, params))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchHelper Created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(LogisticRegression(),\n",
       "  {'penalty': ['l1', 'l2'],\n",
       "   'C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "          5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "          3.59381366e+03, 1.00000000e+04])}),\n",
       " (SVC(),\n",
       "  [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]),\n",
       " (DecisionTreeClassifier(),\n",
       "  {'max_features': ['auto', 'sqrt', 'log2'],\n",
       "   'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       "   'min_samples_leaf': [1],\n",
       "   'random_state': [123]}),\n",
       " (RandomForestClassifier(),\n",
       "  {'n_estimators': [4, 6, 9],\n",
       "   'max_features': ['log2', 'sqrt', 'auto'],\n",
       "   'criterion': ['entropy', 'gini'],\n",
       "   'max_depth': [2, 3, 5, 10],\n",
       "   'min_samples_split': [2, 3, 5],\n",
       "   'min_samples_leaf': [1, 5, 8]})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_grid_1 = GridSearchHelper()\n",
    "temp_grid_1.get_clf_and_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class MyGridSearcher():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clf = None\n",
    "        self.params = None\n",
    "        self.grid_clf = None\n",
    "    \n",
    "    def _get_class_weight(self,y_train):\n",
    "        self.y_train = y_train  \n",
    "        classes = np.unique(self.y_train)\n",
    "        weights = compute_class_weight('balanced', classes = classes, y=self.y_train)\n",
    "        self.class_weight = dict(zip(classes, weights))\n",
    "        \n",
    "        \n",
    "    def get_clf_and_params(self):\n",
    "        \"\"\"\n",
    "        Voting classifier 에 넣을 classifier 들을 리스트로 반환\n",
    "        init_prams 활용\n",
    "        \"\"\"\n",
    "        \n",
    "    def init_params(self,clf, params,cv = 3,scoring = 'accuracy'):\n",
    "        self.clf = clf\n",
    "        self.params = params\n",
    "        self.cv =3 \n",
    "        self.grid_clf = GridSearchCV(clf, params, cv=cv,scoring=scoring,n_jobs=-1,verbose=1)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, X_test, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train        \n",
    "        \n",
    "        self.grid_clf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def get_result_df(self):\n",
    "        \"\"\"\n",
    "        cv_result df\n",
    "        \"\"\"\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        return self.grid_clf.predict_proba(self.X_test)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "mygrid = MyGridSearcher()\n",
    "\n",
    "param_dists ={\n",
    "    'criterion' : ['entropy', 'gini'],\n",
    "    'n_estimators' : [110, 150, 200],\n",
    "    'max_depth':  [10],\n",
    "    'min_samples_leaf' : [1,2,4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#'criterion': ['gini', 'entropy'],\n",
    "#'max_features': ['auto', 'sqrt', 'log2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Class\n"
     ]
    }
   ],
   "source": [
    "X_p_new = PreprocessSelector().strategy(X,strategy_type=\"strategy1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['birth_date',\n",
       " 'age_num',\n",
       " 'graduy_date',\n",
       " 'year_start_date',\n",
       " 'month_start_date',\n",
       " 'tw_hour',\n",
       " 'tw_min',\n",
       " 'workday_num',\n",
       " 'worktime_num',\n",
       " 'worktime_ex_num',\n",
       " 'holy_work_num',\n",
       " 'month_wage_num',\n",
       " 'sat_wage_num',\n",
       " 'sat_stable_num',\n",
       " 'sat_work_num',\n",
       " 'sat_env_num',\n",
       " 'sat_wt_num',\n",
       " 'sat_potential_num',\n",
       " 'sat_relation_num',\n",
       " 'sat_welfare_num',\n",
       " 'sat_hr_num',\n",
       " 'sat_rep1_num',\n",
       " 'sat_auto_num',\n",
       " 'sat_rep2_num',\n",
       " 'sat_fit_num',\n",
       " 'sat_edu_num',\n",
       " 'sat_general_num',\n",
       " 'sat_work-general_num',\n",
       " 'edu-fit_num',\n",
       " 'skill-fit_num',\n",
       " 'major-fit_num',\n",
       " 'major_help_num',\n",
       " 'ins_1_num',\n",
       " 'ins_2_num',\n",
       " 'ins_3_num',\n",
       " 'ins_4_num',\n",
       " 'ins_5_num',\n",
       " 'seeking_time_num',\n",
       " 'health_num',\n",
       " 'drink_num,',\n",
       " 'lifesat_personal',\n",
       " 'lifesat_relational',\n",
       " 'lifesat_group',\n",
       " 'emg_joy_num',\n",
       " 'emg_happy_num',\n",
       " 'emg_comfort_num',\n",
       " 'emb_irr_num',\n",
       " 'emb_negative_num',\n",
       " 'emb_spiritless',\n",
       " 'work_year',\n",
       " 'work_time_num',\n",
       " 'insurances_num',\n",
       " 'biz_sat',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'lifesat',\n",
       " 'benefit_num']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   .named_steps['onehot'].get_feature_names(categorical_features)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_p_new[0].transformers_[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer(fill_value=-2, strategy='constant')),\n",
       "                ('one-hot',\n",
       "                 OneHotEncoder(cols=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                     13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "                                     24, 25, 26],\n",
       "                               handle_unknown='ignore', use_cat_names=True))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_p_new[0].transformers_[1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 하이퍼파라미터 조합으로 만들어진 모델들을 순위별로 나열해 봅니다.\n",
    "# rank_test_score: 테스트 순위\n",
    "# mean_score_time: 예측에 걸리는 시간\n",
    "pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Class\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train.shape[0]\n",
    "\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "feature_col = X.columns\n",
    "\n",
    "X_p = PreprocessSelector().strategy(X,strategy_type=\"strategy1\")\n",
    "\n",
    "X_train_p = X_p[:num_train]\n",
    "X_test_p = X_p[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6348672871423647, 1: 2.3536741214057506}\n"
     ]
    }
   ],
   "source": [
    "mygrid._get_class_weight(y_train)\n",
    "\n",
    "print(mygrid.class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    }
   ],
   "source": [
    "# grid\n",
    "\n",
    "mygrid.init_params(RandomForestClassifier(class_weight=mygrid.class_weight), param_dists,cv=3,scoring='accuracy')\n",
    "\n",
    "\n",
    "mygrid.fit(X_train_p, X_test_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mygrid.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44281417, 0.55718583],\n",
       "       [0.40801169, 0.59198831],\n",
       "       [0.29898196, 0.70101804],\n",
       "       [0.78098981, 0.21901019],\n",
       "       [0.36134781, 0.63865219],\n",
       "       [0.62092994, 0.37907006],\n",
       "       [0.54130788, 0.45869212],\n",
       "       [0.43095219, 0.56904781],\n",
       "       [0.85523216, 0.14476784],\n",
       "       [0.81782742, 0.18217258]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_weights = mygrid.get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_p.shape)\n",
    "print(X_test_p.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=StratifiedKFold(train['QuoteConversion_Flag'], n_folds=5, shuffle=True), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def timer_func(func):\n",
    "    # 실행시간 측정용\n",
    "    # 실행시간을 측정하기 위해 함수를 실행하고 시작 시간과 종료 시간을 저장한다.\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2458226/2323290073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m grcv = GridSearchCV(\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "param_dists ={\n",
    "    'rf__criterion' : ['entropy'],\n",
    "    'rf__n_estimators' : [110, 130],\n",
    "    'rf__max_depth':  [10],\n",
    "    'rf__min_samples_leaf' : [1,2,4]\n",
    "}\n",
    "grcv = GridSearchCV(\n",
    "    pipe, \n",
    "    param_grid=param_dists, \n",
    "    cv=5, \n",
    "    scoring='f1',  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index=X_test.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7367, 160)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature selection과 classification method의 조합\n",
    "- hyperparameter tuning은 greed search cv\n",
    "- permutation importance의 경우 변수간 상관관계를 고려할 필요요가 있음\n",
    "  \n",
    "- feauture의 수를 줄일 것\n",
    "- 다중공선성 검정\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
