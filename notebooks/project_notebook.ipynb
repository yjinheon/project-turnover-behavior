{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019 GOMS 설문조사를 바탕사회 초년생의 이직의도에 영향을 미치는 요인 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install brewer2mpl\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# warning 삭제용\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "large = 22; med = 16; small = 12\n",
    "params = {'axes.titlesize': large,\n",
    "          'legend.fontsize': med,\n",
    "          'figure.figsize': (16, 10),\n",
    "          'axes.labelsize': med,\n",
    "          'axes.titlesize': med,\n",
    "          'xtick.labelsize': med,\n",
    "          'ytick.labelsize': med,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the parent folder path to the sys.path list\n",
    "sys.path.append('../src')\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['g181a297'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.get_dataset_names()\n",
    "#df = sns.load_dataset(\"titanic\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"g181a297\"\n",
    "#target = \"survived\"\n",
    "\n",
    "def split(df, target, test_size=0.3):\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(df, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import OrdinalEncoder\n",
    "# column tranformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict =  {\n",
    "        'g181pid':'id', # id \n",
    "        'g181majorcat':'majorcat', # 전공계열\n",
    "        'g181sex':'sex_cat', # 성별   \n",
    "        'g181birthy':'birth_date', # 출생년\n",
    "        'g181age':'age_num', # 연령\n",
    "        'g181graduy':'graduy_date', #졸업년\n",
    "### 사업체 관련       \n",
    "        'g181a001':'year_start_date', # 현일자리 시작년\n",
    "        'g181a002':'month_start_date', # 현일자리 시작월\n",
    "        'g181a004_10':'ind_cat', # 일자리 산업 대분류\n",
    "        'g181a010':'corp_worker_cat', # 기업체 종사자 수 # 결측값문제로 categorical로 변경\n",
    "        'g181a011':'biz_worker_cat', # 사업체 종사자 수\n",
    "        'g181a018':'tw_hour', # 출근시간_시간\n",
    "        'g181a019':'tw_min', # 출근시간_분\n",
    "        'g181a116':'workday_num', # 주당 정규 근로일\n",
    "        'g181a117':'worktime_num', # 주당정규 근로시간\n",
    "        'g181a118':'worktime_ex_num', # 주당초과 근로시간\n",
    "        'g181a119': 'holy_work_num', # 월평균 휴일근로\n",
    "        'g181a020': 'biztype_cat', # 사업체형태\n",
    "        'g181a022':'regular_cat',     # 정규직 비정규직 여부\n",
    "### 혜택관련\n",
    "        'g181a390':'voluntary_cat',     # 일자리 형태 자발, 비자발 여부\n",
    "        'g181a035':'shift_cat' ,    # 교대제 여부\n",
    "        'g181a038':'pension_cat',     # 퇴직금 제공 여부\n",
    "        'g181a039':'payed_vc_cat',     # 제공여부 2- 유급휴가\n",
    "        'g181a043':'maternity_cat',     #  6- 육아휴직\n",
    "        'g181a045':'overtime_pay_cat',     # 8- 시간 외 수당\n",
    "        'g181a046':'bonus_cat',     # 9- 상여금\n",
    "        'g181a048':'weekly_hl_cat',     # 11- 유급주휴\n",
    "        'g181a392':'baby_vc_cat',     # 12- 산전후휴가\n",
    "        'g181a120':'wage_type_cat',     # 급여 형태 구분\n",
    "        'g181a122':'month_wage_num',     # 월 평균 근로소득\n",
    "        'g181a126':'sat_wage_num',     # 만족도-임금\n",
    "        'g181a127':'sat_stable_num',     # 만족도-고용안정성\n",
    "        'g181a128':'sat_work_num',     # 만족도-직무내용\n",
    "        'g181a129':'sat_env_num',     # 만족도-근무환경\n",
    "        'g181a130':'sat_wt_num',     # 만족도-노동시간\n",
    "        'g181a131':'sat_potential_num',     # 만족도-발전가능성\n",
    "        'g181a132':'sat_relation_num',     # 만족도-인간관계\n",
    "        'g181a133':'sat_welfare_num',     # 만족도-복리후생\n",
    "        'g181a134':'sat_hr_num',     # 만족도-인사체계\n",
    "        'g181a135':'sat_rep1_num',     # 만족도-사회적평판-일\n",
    "        'g181a136':'sat_auto_num',     # 만족도-자율성 및 권한\n",
    "        'g181a137':'sat_rep2_num',     # 만족도-일자리-사회적 평판\n",
    "        'g181a138':'sat_fit_num',     # 만족도-적성흥미일치도\n",
    "        'g181a139':'sat_edu_num',     # 만족도-직무관련 교육\n",
    "        'g181a140':'sat_general_num',     # 만족도-일자리_전반적만족도\n",
    "        'g181a141':'sat_work-general_num',     # 만족도-업무_전반적만족도\n",
    "        'g181a142':'edu-fit_num',     # 교육수준-일수준일치정도\n",
    "        'g181a143':'skill-fit_num',     # 일기술수준-본인기술수준일치정도\n",
    "        'g181a144':'major-fit_num',     # 주전공일치정도\n",
    "        'g181a146':'major_help_num',     # 전공지식업무도움정도\n",
    "        'g181a158':'ins_1_num',     # 보험-국민연금\n",
    "        'g181a159':'ins_2_num',     # 보험-특수직역연금\n",
    "        'g181a160':'ins_3_num',     # 보험-건강보험\n",
    "        'g181a161':'ins_4_num',     # 보험-고용보험\n",
    "        'g181a162':'ins_5_num',     # 보험-산재보험\n",
    "        'g181a189':'seeking_time_num',     # 구직활동경험기간-개월\n",
    "        'g181a283':'adjust_difficulty_cat',     # 다른일자리제의여부\n",
    "        'g181a285':'job_offer_cat',     # 적응시어려움여부\n",
    "        'g181a297':'turnover_intention',     # 이직준비 여부: target\n",
    "        'g181g001':'graduate_cat',     # 대학원 경험유무\n",
    "        'g181l001':'train_cat',     # 취업훈련경험유무\n",
    "        'g181q001':'health_num',     # 현재 견강상태\n",
    "        'g181q004':'smoke_cat',     # 흡연여부\n",
    "        'g181q006':'drink_num,',     # 음주빈도\n",
    "        'g181q015':'lifesat_personal',     # 삶의만족도-개인적 측면\n",
    "        'g181q016':'lifesat_relational',     # 삶의만족도-관계적 측면\n",
    "        'g181q017':'lifesat_group',     # 삶의만족도-소속집단\n",
    "        'g181q018':'emg_joy_num',     # 감정빈도-즐거운\n",
    "        'g181q019':'emg_happy_num',     # 감정빈도-행복한\n",
    "        'g181q020':'emg_comfort_num',     # 감정빈도-편안한\n",
    "        'g181q021':'emb_irr_num',     # 감정빈도-짜증나는\n",
    "        'g181q022':'emb_negative_num',     # 감정빈도-부정적인\n",
    "        'g181q023':'emb_spiritless',     # 감정빈도-무기력한\n",
    "        'g181p001':'marriage_cat',     # 혼인여부\n",
    "        'g181p008':'child_cat',     # 부양자녀 유무\n",
    "        'g181p036':'parent_asset_cat',     # 부모님 자산규모\n",
    "        'g181p046':'livetype_cat',     # 거주형태\n",
    "        'g181p041':'support_cat',      # 경제적 지원여부\n",
    "        # subset에서 추가한 변수들\n",
    "        'worker_type':'worker_type_cat',\n",
    "        'regular_worker':'regular_worker_cat',\n",
    "        'turnover_exp':'turnover_exp_cat',\n",
    "        'work_exp':'work_exp_cat'        \n",
    "}\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# make pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "class Preprocess():\n",
    "    \"\"\"\n",
    "    X_train, X_test\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Preprocessing Class\")\n",
    "        self.numeric_df = None\n",
    "        self.cat_df = None\n",
    "        self.ordinal_col = ['biz_worker_cat','parent_asset_cat','turnover_exp_cat']\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_dtypes(self,data):\n",
    "        # onehot 할 피처\n",
    "        #self.numeric_df = data.select_dtypes(include=['number'])\n",
    "        #self.cat_df = data.select_dtypes(exclude=['number'])\n",
    "        self.cat_col = [col for col in data.columns.to_list() if col.endswith('cat') ] \n",
    "        self.onehot_col = [col for col in self.cat_col if col not in self.ordinal_col]\n",
    "        self.numeric_col = [col for col in data.columns.to_list() if col not in self.onehot_col and col not in self.ordinal_col]\n",
    "        \n",
    "    def col_type_df(self,data):\n",
    "        df = pd.DataFrame\n",
    "        \n",
    "        all_cols = [col for col in data.columns.to_list()]\n",
    "        df = pd.DataFrame({\"col\":all_cols})\n",
    "        \n",
    "        col_cond = [\n",
    "            (df['col'] in self.onehot_col) ,\n",
    "            (df['col'] in self.numeric_col),\n",
    "            (df['col'] in self.ordinal_col)\n",
    "        ]\n",
    "        col_type = [\"category\",\"numeric\",\"ordinal\"]\n",
    "        df[\"type\"] = np.select(col_cond,col_type,default=\"other\")\n",
    "                \n",
    "        return df\n",
    "        \n",
    "    def engineer(self,data):\n",
    "        \n",
    "        # id 칼럼 제거\n",
    "        data = data.drop(['g181pid'], axis=1)\n",
    "    \n",
    "        # 이직경험 빈도 추가\n",
    "        turnover_cond = [\n",
    "        (data['g181d001'].isnull()==True ), # 첫 직장 전 취업한적 없음(알바포함)\n",
    "        (data['g181d001']==1) & (data['g181d006']==1), # 알바 경험 있음\n",
    "        (data['g181d001']==1) & (data['g181d006']==2) & (data['g181e001']==2), # 전직장 있음, 1번이직\n",
    "        (data['g181e001']== 1)] # 전직장 2개 이상 있음\n",
    "        choices = [0, 1, 2, 3]\n",
    "\n",
    "        data['turnover_exp'] = np.select(turnover_cond, choices,default=3)\n",
    "    \n",
    "        data.drop(['g181d001','g181d006','g181e001'], axis=1, inplace=True)\n",
    "\n",
    "        # 알바이외 근로경험 여부\n",
    "        data['work_exp'] = np.where(data['turnover_exp'].isin([0,1]),0,1)\n",
    "    \n",
    "        # 구직활동기간 결측값 0으로 처리\n",
    "    \n",
    "        data['g181a189'] = data['g181a189'].fillna(0)\n",
    "    \n",
    "        # feature name 변경 (map)\n",
    "    \n",
    "        data.columns = data.columns.map(feature_dict)\n",
    "    \n",
    "    \n",
    "        data = data.drop(data[data['month_wage_num'] == -1].index) # 급여 모르는 경우 제거\n",
    "    \n",
    "        data['work_year'] = 2019 - data['year_start_date']  # 근무기간\n",
    "    \n",
    "        data['work_time_num'] = data['tw_min'] + data['tw_hour'] # 출근소요시간\n",
    "    \n",
    "        # 보험 수\n",
    "        insurances_col = [col for col in data if col.startswith('ins')]\n",
    "        data['insurances_num'] = 0\n",
    "        for col in insurances_col:\n",
    "            data['temp'] = np.where(data[col] == 1, 1, 0)\n",
    "            data['insurances_num'] += data['temp']\n",
    "    \n",
    "        # 회사 전반적 만족도\n",
    "        biz_sat_col = [col for col in data if col.startswith('sat')]\n",
    "        data['biz_sat'] = data[biz_sat_col].sum(axis=1)\n",
    "    \n",
    "        # 긍정적 감정\n",
    "        pos_col = [col for col in data if col.startswith('emg') ] \n",
    "        data['pos'] = data[pos_col].sum(axis=1)\n",
    "    \n",
    "        # 부정적 감정\n",
    "        neg_col = [col for col in data if col.startswith('neg') ]\n",
    "        data['neg'] = data[neg_col].sum(axis=1)\n",
    "    \n",
    "        # 삶의 만족도\n",
    "        lifesat_col = [col for col in data if col.startswith('lifesat')]\n",
    "        data['lifesat'] = data[lifesat_col].sum(axis=1)\n",
    "        \n",
    "        # 혜택 수\n",
    "        benefit_col  =['pension_cat',     # 퇴직금 제공 여부\n",
    "            'payed_vc_cat',     # 제공여부 2- 유급휴가\n",
    "            'maternity_cat',     #  6- 육아휴직\n",
    "            'overtime_pay_cat',     # 8- 시간 외 수당\n",
    "            'bonus_cat',     # 9- 상여금\n",
    "            'weekly_hl_cat',     # 11- 유급주휴\n",
    "            'baby_vc_cat'] # 출산휴가\n",
    "\n",
    "        data['benefit_num'] = 0\n",
    "\n",
    "        for cols in benefit_col:\n",
    "            data['temp'] = np.where(data[cols]==1, 1, 0)\n",
    "            data['benefit_num'] += data['temp']\n",
    "        \n",
    "        # 결측값 0으로 처리(구직활동기간이 0이기에)    \n",
    "        data['seeking_time_num'] = data['seeking_time_num'].fillna(0)\n",
    "    \n",
    "        data.drop(columns='temp',axis=1,inplace=True) # temp 삭제\n",
    "        data.drop(data[data.month_wage_num==-1].index,inplace=True) # 급여 모르는 경우 제거\n",
    "    \n",
    "        return data\n",
    "    \n",
    "    def drop_col(self,data,remove_col):\n",
    "        \n",
    "        all_cols = [self.numeric_col,self.onehot_col,self.ordinal_col]\n",
    "        \n",
    "        for col in all_cols:\n",
    "            if remove_col in col:\n",
    "                col.remove(remove_col)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        data.drop(columns = remove_col,axis=1,inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "    def base_pipline(self,data):\n",
    "                \n",
    "        ohe_pipeline = make_pipeline(OneHotEncoder(use_cat_names=True, handle_unknown='ignore'),SimpleImputer(strategy='constant', fill_value=-2))\n",
    "        ord_pipeline = make_pipeline(OrdinalEncoder(handle_unknown='ignore'),SimpleImputer(strategy='constant', fill_value=-2))\n",
    "        num_pipeline = make_pipeline(StandardScaler(),SimpleImputer(strategy='median'))\n",
    "        \n",
    "        preprocess_pipeline = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', num_pipeline, self.numeric_col),\n",
    "                ('ohe', ohe_pipeline, self.onehot_col),\n",
    "                ('ord', ord_pipeline,self.ordinal_col)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #preprocess_pipeline\n",
    "    \n",
    "        return preprocess_pipeline\n",
    "    \n",
    "    \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.DataFrame({'brand': ['aaaa', 'asdfasdf', 'sadfds', 'NaN'],\n",
    "                   'category': ['asdf', 'asfa', 'asdfas', 'as'],\n",
    "                   'num1': [1, 1, 0, 0],\n",
    "                   'target': [0.2, 0.11, 1.34, 1.123]})\n",
    "\n",
    "numeric_features = ['num1']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['brand', 'category']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor',  LinearRegression())])\n",
    "clf.fit(df.drop('target', 1), df['target'])\n",
    "\n",
    "clf.named_steps['preprocessor'].transformers_[1][1]\\\n",
    "   .named_steps['onehot'].get_feature_names(categorical_features)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "        \n",
    "class PreprocessSelector():\n",
    "    \"\"\"\n",
    "    전처리 방식 선택\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data=None\n",
    "        self._preprocessor=Preprocess()\n",
    "\n",
    "    def strategy(self, data, strategy_type=\"strategy1\"):\n",
    "        self.data=data\n",
    "        if strategy_type=='strategy1':\n",
    "            self._strategy1()\n",
    "        elif strategy_type=='strategy2':\n",
    "            self._strategy2()\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def _base_strategy(self):\n",
    "        self.data=self._preprocessor.engineer(self.data)\n",
    "        self._preprocessor.get_dtypes(self.data)\n",
    "\n",
    "    def _strategy1(self):\n",
    "\n",
    "        self._base_strategy()\n",
    "        self.data=self._preprocessor.drop_col(self.data,\"corp_worker_cat\")\n",
    "        self.data=self._preprocessor.base_pipline(self.data)\n",
    "\n",
    "    def _strategy2(self):\n",
    "        \"\"\"\n",
    "        remove features\n",
    "        \"\"\"\n",
    "        self._base_strategy()\n",
    "        #self.data = self._preprocessor.drop_col(self.data,\"corp_worker_cat\")\n",
    "        self.daa = self._preprocessor.drop_col(self.data,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_preprocess = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = ex_preprocess.engineer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_col = ['biz_worker_cat','parent_asset_cat','turnover_exp_cat']\n",
    "cat_col = [col for col in X_train_p.columns.to_list() if col.endswith('cat') ] \n",
    "onehot_col = [col for col in cat_col if col not in ordinal_col]\n",
    "numeric_col = [col for col in X_train_p.columns.to_list() if col not in onehot_col and col not in ordinal_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_features(input_features, n_features_in):\n",
    "    if input_features is not None:\n",
    "        return np.array(input_features, dtype=object)\n",
    "    return np.array([f\"X{i}\" for i in range(n_features_in)], dtype=object)\n",
    "\n",
    "class MySimpleImputer(SimpleImputer):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super(MySimpleImputer, self).__init__(*args, **kwargs)\n",
    "        #self.feature_names_in_ = self.get_feature_names(input_features=None)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.feature_names_in_ = self.get_feature_names(input_features=None)\n",
    "        return super().fit(X, y)\n",
    "        \n",
    "    def get_feature_names(self, input_features=None):\n",
    "        feature_names_in_ = _get_input_features(input_features,self.n_features_in_)\n",
    "        return feature_names_in_\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return self.feature_names_in_\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = SimpleImputer(strategy='median')\n",
    "\n",
    "ms.fit(X_train_p[numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_pipeline\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def base_pipline(data):\n",
    "        SimpleImputer.get_feature_names_out = lambda self, names=None: self.feature_names_in_\n",
    "        \n",
    "        #ord_si=SimpleImputer(strategy='median')\n",
    "        #ord_si.get_feature_names_out = (lambda self, names=None:self.feature_names_in_)\n",
    "        #num_si = SimpleImputer(strategy='median')\n",
    "        #num_si.get_feature_names_out = (lambda self, names=None:self.feature_names_in_)\n",
    "        #ohe_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(use_cat_names=True, handle_unknown='ignore'))\n",
    "        #ord_pipeline = make_pipeline(SimpleImputer(strategy='median'),OrdinalEncoder(handle_unknown='ignore'))\n",
    "        #num_pipeline = make_pipeline(SimpleImputer(strategy='median'),StandardScaler())\n",
    "        \n",
    "        ohe_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                    ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "        ord_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                                    ('ord', OrdinalEncoder(handle_unknown='ignore'))])        \n",
    "        num_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                                    ('scaler', StandardScaler())])\n",
    "        \n",
    "        \n",
    "        preprocess_pipeline = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', num_pipeline, numeric_col),\n",
    "                ('ohe', ohe_pipeline, onehot_col),\n",
    "                ('ord', ord_pipeline, ordinal_col)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        res=preprocess_pipeline\n",
    "        \n",
    "        return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['majorcat', 'sex_cat', 'birth_date', 'age_num', 'graduy_date',\n",
       "       'year_start_date', 'month_start_date', 'ind_cat', 'corp_worker_cat',\n",
       "       'biz_worker_cat', 'tw_hour', 'tw_min', 'workday_num', 'worktime_num',\n",
       "       'worktime_ex_num', 'holy_work_num', 'biztype_cat', 'regular_cat',\n",
       "       'voluntary_cat', 'shift_cat', 'pension_cat', 'payed_vc_cat',\n",
       "       'maternity_cat', 'overtime_pay_cat', 'bonus_cat', 'weekly_hl_cat',\n",
       "       'baby_vc_cat', 'wage_type_cat', 'month_wage_num', 'sat_wage_num',\n",
       "       'sat_stable_num', 'sat_work_num', 'sat_env_num', 'sat_wt_num',\n",
       "       'sat_potential_num', 'sat_relation_num', 'sat_welfare_num',\n",
       "       'sat_hr_num', 'sat_rep1_num', 'sat_auto_num', 'sat_rep2_num',\n",
       "       'sat_fit_num', 'sat_edu_num', 'sat_general_num', 'sat_work-general_num',\n",
       "       'edu-fit_num', 'skill-fit_num', 'major-fit_num', 'major_help_num',\n",
       "       'ins_1_num', 'ins_2_num', 'ins_3_num', 'ins_4_num', 'ins_5_num',\n",
       "       'seeking_time_num', 'adjust_difficulty_cat', 'job_offer_cat',\n",
       "       'graduate_cat', 'train_cat', 'health_num', 'smoke_cat', 'drink_num,',\n",
       "       'lifesat_personal', 'lifesat_relational', 'lifesat_group',\n",
       "       'emg_joy_num', 'emg_happy_num', 'emg_comfort_num', 'emb_irr_num',\n",
       "       'emb_negative_num', 'emb_spiritless', 'marriage_cat', 'child_cat',\n",
       "       'parent_asset_cat', 'livetype_cat', 'support_cat', 'worker_type_cat',\n",
       "       'regular_worker_cat', 'turnover_exp_cat', 'work_exp_cat', 'work_year',\n",
       "       'work_time_num', 'insurances_num', 'biz_sat', 'pos', 'neg', 'lifesat',\n",
       "       'benefit_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['birth_date', 'age_num', 'graduy_date',\n",
       "                                  'year_start_date', 'month_start_date',\n",
       "                                  'tw_hour', 'tw_min', 'workday_num',\n",
       "                                  'worktime_num', 'worktime_ex_num',\n",
       "                                  'holy_work_num', 'month_wage_num',\n",
       "                                  'sat_wage_num', 'sat_stable_num',\n",
       "                                  'sat_work_num'...\n",
       "                                  'wage_type_cat', 'adjust_difficulty_cat',\n",
       "                                  'job_offer_cat', 'graduate_cat', 'train_cat',\n",
       "                                  'smoke_cat', 'marriage_cat', 'child_cat',\n",
       "                                  'livetype_cat', 'support_cat',\n",
       "                                  'worker_type_cat', 'regular_worker_cat',\n",
       "                                  'work_exp_cat']),\n",
       "                                ('ord',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('ord',\n",
       "                                                  OrdinalEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['biz_worker_cat', 'parent_asset_cat',\n",
       "                                  'turnover_exp_cat'])])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = base_pipline(X_train_p)\n",
    "\n",
    "pp.fit(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_input=pp.fit_transform(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7367, 170)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimpy import clean_columns\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "def get_column_names_from_ColumnTransformer(column_transformer, clean_column_names=True, verbose=True):  \n",
    "\n",
    "    \"\"\"\n",
    "    Reference: Kyle Gilde: https://github.com/kylegilde/Kaggle-Notebooks/blob/master/Extracting-and-Plotting-Scikit-Feature-Names-and-Importances/feature_importance.py\n",
    "    Description: Get the column names from the a ColumnTransformer containing transformers & pipelines\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose: Bool indicating whether to print summaries. Default set to True.\n",
    "    Returns\n",
    "    -------\n",
    "    a list of the correct feature names\n",
    "    Note:\n",
    "    If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns,\n",
    "    it must come last in the pipeline. For example, OneHotEncoder, MissingIndicator & SimpleImputer(add_indicator=True) add columns\n",
    "    to the dataset that didn't exist before, so there should come last in the Pipeline.\n",
    "    Inspiration: https://github.com/scikit-learn/scikit-learn/issues/12525\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(column_transformer, ColumnTransformer), \"Input isn't a ColumnTransformer\"\n",
    "    \n",
    "    check_is_fitted(column_transformer)\n",
    "\n",
    "    new_feature_names, transformer_list = [], []\n",
    "\n",
    "    for i, transformer_item in enumerate(column_transformer.transformers_): \n",
    "        transformer_name, transformer, orig_feature_names = transformer_item\n",
    "        orig_feature_names = list(orig_feature_names)\n",
    "\n",
    "        if len(orig_feature_names) == 0:\n",
    "            continue\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"\\n\\n{i}.Transformer/Pipeline: {transformer_name} {transformer.__class__.__name__}\\n\")\n",
    "            print(f\"\\tn_orig_feature_names:{len(orig_feature_names)}\")\n",
    "\n",
    "        if transformer == 'drop':\n",
    "            continue\n",
    "\n",
    "        if isinstance(transformer, Pipeline):\n",
    "            # if pipeline, get the last transformer in the Pipeline\n",
    "            transformer = transformer.steps[-1][1]\n",
    "\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            if 'input_features' in transformer.get_feature_names_out.__code__.co_varnames:\n",
    "                names = list(transformer.get_feature_names_out(orig_feature_names))\n",
    "            else:\n",
    "                names = list(transformer.get_feature_names_out())\n",
    "        elif hasattr(transformer, 'get_feature_names'):\n",
    "            if 'input_features' in transformer.get_feature_names.__code__.co_varnames:\n",
    "                names = list(transformer.get_feature_names(orig_feature_names))\n",
    "            else:\n",
    "                names = list(transformer.get_feature_names())\n",
    "\n",
    "        elif hasattr(transformer,'indicator_') and transformer.add_indicator:\n",
    "            # is this transformer one of the imputers & did it call the MissingIndicator?\n",
    "\n",
    "            missing_indicator_indices = transformer.indicator_.features_\n",
    "            missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n",
    "                                  for idx in missing_indicator_indices]\n",
    "            names = orig_feature_names + missing_indicators\n",
    "\n",
    "        elif hasattr(transformer,'features_'):\n",
    "            # is this a MissingIndicator class? \n",
    "            missing_indicator_indices = transformer.features_\n",
    "            missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n",
    "                                  for idx in missing_indicator_indices]\n",
    "\n",
    "        else:\n",
    "\n",
    "            names = orig_feature_names\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"\\tn_new_features:{len(names)}\")\n",
    "            print(f\"\\tnew_features: {names}\\n\")\n",
    "\n",
    "        new_feature_names.extend(names)\n",
    "        transformer_list.extend([transformer_name] * len(names))\n",
    "\n",
    "    transformer_list, column_transformer_features = transformer_list, new_feature_names\n",
    "\n",
    "    if clean_column_names:\n",
    "        new_feature_names = list(clean_columns(pd.DataFrame(columns=new_feature_names)).columns)\n",
    "    \n",
    "    return new_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.Transformer/Pipeline: num Pipeline\n",
      "\n",
      "\tn_orig_feature_names:57\n",
      "\tn_new_features:57\n",
      "\tnew_features: ['birth_date', 'age_num', 'graduy_date', 'year_start_date', 'month_start_date', 'tw_hour', 'tw_min', 'workday_num', 'worktime_num', 'worktime_ex_num', 'holy_work_num', 'month_wage_num', 'sat_wage_num', 'sat_stable_num', 'sat_work_num', 'sat_env_num', 'sat_wt_num', 'sat_potential_num', 'sat_relation_num', 'sat_welfare_num', 'sat_hr_num', 'sat_rep1_num', 'sat_auto_num', 'sat_rep2_num', 'sat_fit_num', 'sat_edu_num', 'sat_general_num', 'sat_work-general_num', 'edu-fit_num', 'skill-fit_num', 'major-fit_num', 'major_help_num', 'ins_1_num', 'ins_2_num', 'ins_3_num', 'ins_4_num', 'ins_5_num', 'seeking_time_num', 'health_num', 'drink_num,', 'lifesat_personal', 'lifesat_relational', 'lifesat_group', 'emg_joy_num', 'emg_happy_num', 'emg_comfort_num', 'emb_irr_num', 'emb_negative_num', 'emb_spiritless', 'work_year', 'work_time_num', 'insurances_num', 'biz_sat', 'pos', 'neg', 'lifesat', 'benefit_num']\n",
      "\n",
      "\n",
      "\n",
      "1.Transformer/Pipeline: ohe Pipeline\n",
      "\n",
      "\tn_orig_feature_names:28\n",
      "\tn_new_features:110\n",
      "\tnew_features: ['majorcat_1', 'majorcat_2', 'majorcat_3', 'majorcat_4', 'majorcat_5', 'majorcat_6', 'majorcat_7', 'sex_cat_1', 'sex_cat_2', 'ind_cat_-1.0', 'ind_cat_1.0', 'ind_cat_2.0', 'ind_cat_3.0', 'ind_cat_4.0', 'ind_cat_5.0', 'ind_cat_6.0', 'ind_cat_7.0', 'ind_cat_8.0', 'ind_cat_9.0', 'ind_cat_10.0', 'ind_cat_11.0', 'ind_cat_12.0', 'ind_cat_13.0', 'ind_cat_14.0', 'ind_cat_15.0', 'ind_cat_16.0', 'ind_cat_17.0', 'ind_cat_18.0', 'ind_cat_19.0', 'ind_cat_21.0', 'corp_worker_cat_-1.0', 'corp_worker_cat_1.0', 'corp_worker_cat_2.0', 'corp_worker_cat_3.0', 'corp_worker_cat_4.0', 'corp_worker_cat_5.0', 'corp_worker_cat_6.0', 'corp_worker_cat_7.0', 'corp_worker_cat_8.0', 'corp_worker_cat_9.0', 'biztype_cat_1.0', 'biztype_cat_2.0', 'biztype_cat_3.0', 'biztype_cat_4.0', 'biztype_cat_5.0', 'biztype_cat_6.0', 'biztype_cat_7.0', 'biztype_cat_9.0', 'regular_cat_1.0', 'regular_cat_2.0', 'voluntary_cat_1.0', 'voluntary_cat_2.0', 'shift_cat_1.0', 'shift_cat_2.0', 'pension_cat_1.0', 'pension_cat_2.0', 'pension_cat_3.0', 'payed_vc_cat_1.0', 'payed_vc_cat_2.0', 'payed_vc_cat_3.0', 'maternity_cat_1.0', 'maternity_cat_2.0', 'maternity_cat_3.0', 'overtime_pay_cat_1.0', 'overtime_pay_cat_2.0', 'overtime_pay_cat_3.0', 'bonus_cat_1.0', 'bonus_cat_2.0', 'bonus_cat_3.0', 'weekly_hl_cat_1.0', 'weekly_hl_cat_2.0', 'weekly_hl_cat_3.0', 'baby_vc_cat_1.0', 'baby_vc_cat_2.0', 'baby_vc_cat_3.0', 'wage_type_cat_1.0', 'wage_type_cat_2.0', 'wage_type_cat_3.0', 'wage_type_cat_4.0', 'wage_type_cat_5.0', 'adjust_difficulty_cat_1.0', 'adjust_difficulty_cat_2.0', 'job_offer_cat_1.0', 'job_offer_cat_2.0', 'graduate_cat_1', 'graduate_cat_2', 'train_cat_1', 'train_cat_2', 'smoke_cat_1', 'smoke_cat_2', 'marriage_cat_1', 'marriage_cat_2', 'marriage_cat_3', 'marriage_cat_4', 'child_cat_-1', 'child_cat_1', 'child_cat_2', 'livetype_cat_1', 'livetype_cat_2', 'livetype_cat_3', 'livetype_cat_4', 'livetype_cat_5', 'livetype_cat_6', 'support_cat_-1', 'support_cat_1', 'support_cat_2', 'worker_type_cat_employed', 'regular_worker_cat_yes', 'work_exp_cat_0', 'work_exp_cat_1']\n",
      "\n",
      "\n",
      "\n",
      "2.Transformer/Pipeline: ord Pipeline\n",
      "\n",
      "\tn_orig_feature_names:3\n",
      "\tn_new_features:3\n",
      "\tnew_features: [0, 1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_feat=get_column_names_from_ColumnTransformer(pp, clean_column_names=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat[-3:] = ['biz_worker_cat', 'parent_asset_cat',\n",
    "                                  'turnover_exp_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pp.fit_transform(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fin = pd.DataFrame(res, columns=new_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birth_date</th>\n",
       "      <th>age_num</th>\n",
       "      <th>graduy_date</th>\n",
       "      <th>year_start_date</th>\n",
       "      <th>month_start_date</th>\n",
       "      <th>tw_hour</th>\n",
       "      <th>tw_min</th>\n",
       "      <th>workday_num</th>\n",
       "      <th>worktime_num</th>\n",
       "      <th>worktime_ex_num</th>\n",
       "      <th>...</th>\n",
       "      <th>support_cat_-1</th>\n",
       "      <th>support_cat_1</th>\n",
       "      <th>support_cat_2</th>\n",
       "      <th>worker_type_cat_employed</th>\n",
       "      <th>regular_worker_cat_yes</th>\n",
       "      <th>work_exp_cat_0</th>\n",
       "      <th>work_exp_cat_1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.646717</td>\n",
       "      <td>0.634509</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>-1.461757</td>\n",
       "      <td>1.309885</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>1.436492</td>\n",
       "      <td>-3.334144</td>\n",
       "      <td>1.278570</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124299</td>\n",
       "      <td>-0.137394</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>-0.439713</td>\n",
       "      <td>1.891083</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>-0.685226</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.381304</td>\n",
       "      <td>-0.480462</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.582331</td>\n",
       "      <td>-0.433709</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>0.729252</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>1.278570</td>\n",
       "      <td>-0.220429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.132706</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-0.433709</td>\n",
       "      <td>1.801296</td>\n",
       "      <td>-1.392466</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>0.714029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.389712</td>\n",
       "      <td>0.462975</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-1.305506</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>0.375633</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>1.087812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7362</th>\n",
       "      <td>0.124299</td>\n",
       "      <td>-0.158835</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-0.433709</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>-1.321742</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7363</th>\n",
       "      <td>-0.132706</td>\n",
       "      <td>0.012699</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-1.305506</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>0.729252</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>1.648487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td>-0.132706</td>\n",
       "      <td>0.184233</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-1.305506</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>-0.331607</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>0.381304</td>\n",
       "      <td>-0.308928</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-0.724308</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.138308</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>0.381304</td>\n",
       "      <td>-0.437578</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.071309</td>\n",
       "      <td>-1.305506</td>\n",
       "      <td>-0.498556</td>\n",
       "      <td>1.436492</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-2.263624</td>\n",
       "      <td>-0.594212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7367 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      birth_date   age_num  graduy_date  year_start_date  month_start_date  \\\n",
       "0      -0.646717  0.634509     0.477105        -1.461757          1.309885   \n",
       "1       0.124299 -0.137394     0.477105        -0.439713          1.891083   \n",
       "2       0.381304 -0.480462     0.477105         0.582331         -0.433709   \n",
       "3      -0.132706  0.034140     0.477105         0.071309         -0.433709   \n",
       "4      -0.389712  0.462975     0.477105         0.071309         -1.305506   \n",
       "...          ...       ...          ...              ...               ...   \n",
       "7362    0.124299 -0.158835     0.477105         0.071309         -0.433709   \n",
       "7363   -0.132706  0.012699     0.477105         0.071309         -1.305506   \n",
       "7364   -0.132706  0.184233     0.477105         0.071309         -1.305506   \n",
       "7365    0.381304 -0.308928     0.477105         0.071309         -0.724308   \n",
       "7366    0.381304 -0.437578     0.477105         0.071309         -1.305506   \n",
       "\n",
       "       tw_hour    tw_min  workday_num  worktime_num  worktime_ex_num  ...  \\\n",
       "0    -0.498556  1.436492    -3.334144      1.278570        -0.594212  ...   \n",
       "1    -0.498556 -0.685226    -0.124181     -0.138308        -0.594212  ...   \n",
       "2    -0.498556  0.729252    -0.124181      1.278570        -0.220429  ...   \n",
       "3     1.801296 -1.392466    -0.124181     -0.138308         0.714029  ...   \n",
       "4    -0.498556  0.375633    -0.124181     -0.138308         1.087812  ...   \n",
       "...        ...       ...          ...           ...              ...  ...   \n",
       "7362 -0.498556 -1.321742    -0.124181     -0.138308        -0.594212  ...   \n",
       "7363 -0.498556  0.729252    -0.124181     -0.138308         1.648487  ...   \n",
       "7364 -0.498556 -0.331607    -0.124181     -0.138308        -0.594212  ...   \n",
       "7365 -0.498556  0.022013    -0.124181     -0.138308        -0.594212  ...   \n",
       "7366 -0.498556  1.436492    -0.124181     -2.263624        -0.594212  ...   \n",
       "\n",
       "      support_cat_-1  support_cat_1  support_cat_2  worker_type_cat_employed  \\\n",
       "0                0.0            0.0            1.0                       1.0   \n",
       "1                0.0            0.0            1.0                       1.0   \n",
       "2                0.0            0.0            1.0                       1.0   \n",
       "3                0.0            0.0            1.0                       1.0   \n",
       "4                0.0            0.0            1.0                       1.0   \n",
       "...              ...            ...            ...                       ...   \n",
       "7362             0.0            0.0            1.0                       1.0   \n",
       "7363             0.0            0.0            1.0                       1.0   \n",
       "7364             0.0            0.0            1.0                       1.0   \n",
       "7365             0.0            0.0            1.0                       1.0   \n",
       "7366             0.0            0.0            1.0                       1.0   \n",
       "\n",
       "      regular_worker_cat_yes  work_exp_cat_0  work_exp_cat_1    0    1    2  \n",
       "0                        1.0             1.0             0.0  5.0  4.0  0.0  \n",
       "1                        1.0             1.0             0.0  7.0  5.0  0.0  \n",
       "2                        1.0             0.0             1.0  3.0  3.0  2.0  \n",
       "3                        1.0             1.0             0.0  4.0  4.0  0.0  \n",
       "4                        1.0             1.0             0.0  4.0  2.0  0.0  \n",
       "...                      ...             ...             ...  ...  ...  ...  \n",
       "7362                     1.0             1.0             0.0  1.0  5.0  0.0  \n",
       "7363                     1.0             1.0             0.0  8.0  1.0  0.0  \n",
       "7364                     1.0             1.0             0.0  6.0  2.0  0.0  \n",
       "7365                     1.0             1.0             0.0  5.0  1.0  0.0  \n",
       "7366                     1.0             1.0             0.0  1.0  6.0  0.0  \n",
       "\n",
       "[7367 rows x 170 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyPipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "#model0 = GradientBoostingClassifier(random_state=42)\n",
    "#model0.fit(X_train, y_train)\n",
    "#def get_roc_auc(model, X, y):\n",
    "#    y_proba = model.predict_proba(X)[:,1]\n",
    "#    return roc_auc_score(y, y_proba)\n",
    "#print(f\"Training data - ROC AUC: {get_roc_auc(model0, X_train, y_train):.4f}\")\n",
    "#print(f\"Test data - ROC AUC: {get_roc_auc(model0, X_test, y_test):.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class BaseModel: # Don't use parentheses unless you are subclassing other classes.\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clf=SVC(probability=True)\n",
    "        \n",
    "    def get_baseline(self,X,y):\n",
    "        y_common = y.mode()[0]\n",
    "        y_train_pred = y_common * np.ones(X.shape[0])\n",
    "        return y_train_pred\n",
    "    \n",
    "        \n",
    "    def get_roc_auc(self,X,y):\n",
    "        y_proba = self.clf.predict_proba(X)[:,1]\n",
    "        return roc_auc_score(y, y_proba)\n",
    "    \n",
    "    def get_classification_report(self,X,y):\n",
    "        y_pred = self.clf.predict(X)\n",
    "        return print(classification_report(y, y_pred))\n",
    "\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(fin_input, LogisticRegression())\n",
    "\n",
    "#pipeline.fit(X_train_p, y_train)\n",
    "\n",
    "\n",
    "## get pipeline feature names\n",
    "#pipeline[:-1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90      5802\n",
      "           1       0.89      0.26      0.40      1565\n",
      "\n",
      "    accuracy                           0.84      7367\n",
      "   macro avg       0.86      0.62      0.65      7367\n",
      "weighted avg       0.84      0.84      0.80      7367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bass_model = BaseModel()\n",
    "bass_model.fit(fin_input,y_train)\n",
    "bass_model.get_classification_report(fin_input,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bass_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearchHelper():\n",
    "    def __init__(self):\n",
    "        print(\"GridSearchHelper Created\")\n",
    "\n",
    "        self.gridSearchCV=None\n",
    "        self.clf_and_params=[]\n",
    "        self._initialize_clf_and_params()\n",
    "\n",
    "    def _initialize_clf_and_params(self):\n",
    "        \n",
    "        clf=LogisticRegression()\n",
    "        params={'penalty':['l1', 'l2'],\n",
    "                'C':np.logspace(0, 4, 10)\n",
    "                }\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "        clf = SVC()\n",
    "        params = [ {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "                   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "        clf=DecisionTreeClassifier()\n",
    "        params={'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "          'min_samples_leaf':[1],\n",
    "          'random_state':[123]}\n",
    "        #Because of depricating warning for Decision Tree which is not appended.\n",
    "        #But it give high competion accuracy score. You can append when you run the kernel\n",
    "        self.clf_and_params.append((clf,params))\n",
    "\n",
    "        clf = RandomForestClassifier()\n",
    "        params = {'n_estimators': [4, 6, 9],\n",
    "              'max_features': ['log2', 'sqrt','auto'],\n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10],\n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "        #Because of depricating warning for RandomForestClassifier which is not appended.\n",
    "        #But it give high competion accuracy score. You can append when you run the kernel\n",
    "        self.clf_and_params.append((clf, params))\n",
    "\n",
    "    def fit_predict_save(self, X_train, X_test, y_train,strategy_type):\n",
    "        self.X_train=X_train\n",
    "        self.X_test=X_test\n",
    "        self.y_train=y_train\n",
    "        self.strategy_type=strategy_type\n",
    "\n",
    "        clf_and_params = self.get_clf_and_params()\n",
    "        models=[]\n",
    "        self.results={}\n",
    "        for clf, params in clf_and_params:\n",
    "            self.current_clf_name = clf.__class__.__name__\n",
    "            grid_search_clf = GridSearchCV(clf, params, cv=5)\n",
    "            grid_search_clf.fit(self.X_train, self.y_train)\n",
    "            self.Y_pred = grid_search_clf.predict(self.X_test)\n",
    "            clf_train_acc = round(grid_search_clf.score(self.X_train, self.y_train) * 100, 2)\n",
    "            print(self.current_clf_name, \" trained and used for prediction on test data...\")\n",
    "            self.results[self.current_clf_name]=clf_train_acc\n",
    "            # for ensemble\n",
    "            models.append(clf)\n",
    "\n",
    "            self.save_result()\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    def show_result(self):\n",
    "        for clf_name, train_acc in self.results.items():\n",
    "                  print(\"{} train accuracy is {:.3f}\".format(clf_name, train_acc))\n",
    "        \n",
    "    def save_result(self):\n",
    "        id_idx = list(range(1, len(self.Y_pred) + 1))\n",
    "        Submission = pd.DataFrame({'Id': id_idx,\n",
    "                                    'Survived': self.Y_pred})\n",
    "        file_name=\"{}_{}.csv\".format(self.strategy_type,self.current_clf_name.lower())\n",
    "        Submission.to_csv(file_name, index=False)\n",
    "\n",
    "        print(\"Submission saved file name: \",file_name)\n",
    "\n",
    "    def get_clf_and_params(self):\n",
    "\n",
    "        return self.clf_and_params\n",
    "\n",
    "    def add(self,clf, params):\n",
    "        self.clf_and_params.append((clf, params))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grid_1 = GridSearchHelper()\n",
    "temp_grid_1.get_clf_and_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class MyGridSearcher():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clf = None\n",
    "        self.params = None\n",
    "        self.grid_clf = None\n",
    "    \n",
    "    def _get_class_weight(self,y_train):\n",
    "        self.y_train = y_train  \n",
    "        classes = np.unique(self.y_train)\n",
    "        weights = compute_class_weight('balanced', classes = classes, y=self.y_train)\n",
    "        self.class_weight = dict(zip(classes, weights))\n",
    "        \n",
    "        \n",
    "    def get_clf_and_params(self):\n",
    "        \"\"\"\n",
    "        Voting classifier 에 넣을 classifier 들을 리스트로 반환\n",
    "        init_prams 활용\n",
    "        \"\"\"\n",
    "        \n",
    "    def init_params(self,clf, params,cv = 3,scoring = 'accuracy'):\n",
    "        self.clf = clf\n",
    "        self.params = params\n",
    "        self.cv =3 \n",
    "        self.grid_clf = GridSearchCV(clf, params, cv=cv,scoring=scoring,n_jobs=-1,verbose=1)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, X_test, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train        \n",
    "        \n",
    "        self.grid_clf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def get_result_df(self):\n",
    "        \"\"\"\n",
    "        cv_result df\n",
    "        \"\"\"\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        return self.grid_clf.predict_proba(self.X_test)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "mygrid = MyGridSearcher()\n",
    "\n",
    "param_dists ={\n",
    "    'criterion' : ['entropy', 'gini'],\n",
    "    'n_estimators' : [110, 150, 200],\n",
    "    'max_depth':  [10],\n",
    "    'min_samples_leaf' : [1,2,4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#'criterion': ['gini', 'entropy'],\n",
    "#'max_features': ['auto', 'sqrt', 'log2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p_new = PreprocessSelector().strategy(X,strategy_type=\"strategy1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   .named_steps['onehot'].get_feature_names(categorical_features)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_p_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p_new.named_steps['columntransformer'].fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val_transformed = gr_rfc.named_steps['preprocessing'].transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 하이퍼파라미터 조합으로 만들어진 모델들을 순위별로 나열해 봅니다.\n",
    "# rank_test_score: 테스트 순위\n",
    "# mean_score_time: 예측에 걸리는 시간\n",
    "pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "feature_col = X.columns\n",
    "\n",
    "X_p = PreprocessSelector().strategy(X,strategy_type=\"strategy1\")\n",
    "\n",
    "X_train_p = X_p[:num_train]\n",
    "X_test_p = X_p[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygrid._get_class_weight(y_train)\n",
    "\n",
    "print(mygrid.class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid\n",
    "\n",
    "mygrid.init_params(RandomForestClassifier(class_weight=mygrid.class_weight), param_dists,cv=3,scoring='accuracy')\n",
    "\n",
    "\n",
    "mygrid.fit(X_train_p, X_test_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mygrid.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_weights = mygrid.get_class_weight(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_p.shape)\n",
    "print(X_test_p.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=StratifiedKFold(train['QuoteConversion_Flag'], n_folds=5, shuffle=True), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def timer_func(func):\n",
    "    # 실행시간 측정용\n",
    "    # 실행시간을 측정하기 위해 함수를 실행하고 시작 시간과 종료 시간을 저장한다.\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dists ={\n",
    "    'rf__criterion' : ['entropy'],\n",
    "    'rf__n_estimators' : [110, 130],\n",
    "    'rf__max_depth':  [10],\n",
    "    'rf__min_samples_leaf' : [1,2,4]\n",
    "}\n",
    "grcv = GridSearchCV(\n",
    "    pipe, \n",
    "    param_grid=param_dists, \n",
    "    cv=5, \n",
    "    scoring='f1',  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index=X_test.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature selection과 classification method의 조합\n",
    "- hyperparameter tuning은 greed search cv\n",
    "- permutation importance의 경우 변수간 상관관계를 고려할 필요요가 있음\n",
    "  \n",
    "- feauture의 수를 줄일 것\n",
    "- 다중공선성 검정\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
